{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Workflow (without parallelization)\n",
    "\n",
    "This document outlines the PDG workflow, essentially parsing [this](https://github.com/PermafrostDiscoveryGateway/viz-workflow/blob/parsl-workflow/pdg_workflow/pdg_workflow.py) workflow but omitting parallelization.\n",
    "\n",
    "### Recommendations to improve onbarding process with Ingmar's sample data:\n",
    "- create repo for sample data processing from the start with just 1 option for `workflow_config.json` file\n",
    "- recommend using `pip install` for all packages, rather than mixing `conda forge` and `pip install`\n",
    "- template code for entire workflow is not parallelized at the start with `parsl`\n",
    "    - parsing this code helped me understand it more, but starting unparallelized and then adding in batching, logging, and `parsl` _after_ is much more intuitive)\n",
    "- keep variable names consistent within template code (for example, choose either `tiles3dmaker` or `converter3d`)\n",
    "- include recommendations for checks along the way that the steps for staging, rasterization, and 3dtile creation were executed correctly, such as the number of `staged` files that should have been generated, the ratio of the number of files within the subfolders of the `geotiff` dir between all zoom levels, etc.\n",
    "- **clarify difference between creating webtiles by _just_ using rasterizer.rasterize_all() versus creating webtiles by running that function _and_ subsequently running function rasterizer.webtiles_from_geotiffs()**\n",
    "    - see slack message & issue\n",
    "- define relationship between update_ranges parameter and the config file, for example by running `rasterize_all()`, we are updating the ranges when creating the webtiles because that wraps around both `rasterize_vectors()` and `webtiles_from_all_geotiffs()`, and the latter has the default `update_ranges=True`, so if we use `rasterize_all()` (not batching and parallelizing) we do not need to manually run `rasterizer.update_ranges()`, but if we just use rasterize() (with batching and parallelization) we do need to run `update_ranges()`\n",
    "\n",
    "### Ideas for future improvements\n",
    "\n",
    "- can create GitHub issues about them in the approproate PDG repositories\n",
    "- add progress outputs for rasterization step (ex: \"Rasterizing z-level 11\") or a progress bar that populates in the terminal with the percentage complete\n",
    "- import StagedTo3dConverter instead of having it within the script, Robyn mentioned this is a bug she encountered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "# file paths\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import box\n",
    "\n",
    "# PDG packages\n",
    "import pdgstaging\n",
    "import pdgraster\n",
    "import py3dtiles\n",
    "import viz_3dtiles\n",
    "from viz_3dtiles import TreeGenerator, BoundingVolumeRegion\n",
    "#import pdgpy3dtiles\n",
    "#from StagedTo3DConverter import StagedTo3DConverter\n",
    "\n",
    "# logging and configuration\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import logging.config\n",
    "import argparse\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define variables and configuration files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data: sample of lake change data from Ingmar\n",
    "#input = '/home/pdg/data/nitze_lake_change/data_sample_2022-09-09/32607/05_Lake_Dataset_Raster_02_final/lake_change.gpkg'\n",
    "base_dir = Path('/home/pdg/data/nitze_lake_change/data_sample_2022-09-09')\n",
    "subdirs = ['32607', '32608', '32609']\n",
    "filename = 'lake_change.gpkg'\n",
    "# to define each .gpkg file within each UTM subdir as a string representation with forward slashes, use as_posix() for each iteration\n",
    "# of base_dir + filename. The ** represents that any subdir string can be present between the base_dir and the filename, meaning I do not\n",
    "# think that we needed to create the object subdirs above\n",
    "data_paths = [p.as_posix() for p in base_dir.glob('**/' + filename)]\n",
    "\n",
    "#workflow_config = '/home/jcohen/lake_change_sample/ingmar-config.json'\n",
    "workflow_config = {\n",
    "    'simplify_tolerance': 0.0001,\n",
    "    'tms_id': 'WorldCRS84Quad',\n",
    "    'z_range': (0, 11),\n",
    "    'tile_size': (256, 256),\n",
    "    'statistics': [\n",
    "        {\n",
    "            'name': 'polygon_count',\n",
    "            'weight_by': 'count',\n",
    "            'property': 'centroids_per_pixel',\n",
    "            'aggregation_method': 'sum',\n",
    "            'resampling_method': 'sum',\n",
    "            'val_range': [0, None],\n",
    "            'nodata_val': 0,\n",
    "            'nodata_color': '#ffffff00',\n",
    "            'palette': ['#d9c43f', '#d93fce']\n",
    "        },\n",
    "        {\n",
    "            'name': 'coverage',\n",
    "            'weight_by': 'area',\n",
    "            'property': 'area_per_pixel_area',\n",
    "            'aggregation_method': 'sum',\n",
    "            'resampling_method': 'average',\n",
    "            'val_range': [0, 1],\n",
    "            'nodata_val': 0,\n",
    "            'nodata_color': '#ffffff00',\n",
    "            'palette': ['#d9c43f', '#d93fce']\n",
    "        }\n",
    "    ],\n",
    "    'deduplicate_at': ['raster'],\n",
    "    'deduplicate_method': 'neighbor',\n",
    "    'deduplicate_keep_rules': [['staging_filename', 'larger']],\n",
    "    'deduplicate_overlap_tolerance': 0.1,\n",
    "    'deduplicate_overlap_both': False,\n",
    "    'deduplicate_centroid_tolerance': None\n",
    "}\n",
    "\n",
    "# How above config differs from ingmar-config.json:\n",
    "# z_range is in () instead of []\n",
    "\n",
    "logging_config = '/home/jcohen/lake_change_sample/logging.json'\n",
    "\n",
    "# not batching in this script so don't need to define the following\n",
    "# the following values are the defaults in the custom function `run_pdg_workflow()`\n",
    "#batch_size_staging=1\n",
    "#batch_size_rasterization=30\n",
    "#batch_size_3dtiles=20\n",
    "#batch_size_parent_3dtiles=500\n",
    "#batch_size_geotiffs=200\n",
    "#batch_size_web_tiles=200\n",
    "\n",
    "# track events that happen as software is executed, helpful for debugging, this came from StagedTo3DConverter.py\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/pdg/data/nitze_lake_change/data_sample_2022-09-09/32609/05_Lake_Dataset_Raster_02_final/lake_change.gpkg',\n",
       " '/home/pdg/data/nitze_lake_change/data_sample_2022-09-09/32608/05_Lake_Dataset_Raster_02_final/lake_change.gpkg',\n",
       " '/home/pdg/data/nitze_lake_change/data_sample_2022-09-09/32607/05_Lake_Dataset_Raster_02_final/lake_change.gpkg']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see data paths\n",
    "data_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging(log_json_file):\n",
    "    \"\"\"\n",
    "    Setup logging configuration\n",
    "    \"\"\"\n",
    "    with open(log_json_file, 'r') as f:\n",
    "        logging_dict = json.load(f)\n",
    "    logging.config.dictConfig(logging_dict)\n",
    "    return logging_dict\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "\n",
    "#     parser = argparse.ArgumentParser(\n",
    "#         description='Run the PDG visualization workflow.')\n",
    "#     parser.add_argument('-c', '--config',\n",
    "#                         help='Path to the pdg-viz configuration JSON file.',\n",
    "#                         default='config.json',\n",
    "#                         type=str)\n",
    "#     parser.add_argument('-l', '--logging',\n",
    "#                         help='Path to the logging configuration JSON file.',\n",
    "#                         default='logging.json',\n",
    "#                         type=str)\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "logging_dict = setup_logging(logging_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: define custom class and methods to be used throughout workflow\n",
    "\n",
    "##### Define a class that orchestrates viz-3dtiles classes and communicates information between them\n",
    "\n",
    "- `__init__` is used to create an object from a class, it is only used within classes\n",
    "- using `__init__` is the \"constructor method\" that is derived from C++ and Java\n",
    "- functions defined within a class are called methods\n",
    "- using `__init__` results in the methods being automatically applied to any object that is created of the class `StagedTo3DConverter`\n",
    "- essentially, this means that when an object of class `StagedTo3DConverter` is created, the configuration .json file is automatically applied to that object\n",
    "- the methods that are defined after this initiation happens do not automatically occur, they must be deliberately applied, which we do later in the workflow by running `class.method()` to create the ceisum 3d files from the satged directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StagedTo3DConverter():\n",
    "    \"\"\"\n",
    "        Processes staged vector data into Cesium 3D tiles according to the\n",
    "        settings in a config file or dict. This class acts as the orchestrator\n",
    "        of the other viz-3dtiles classes, and coordinates the sending and\n",
    "        receiving of information between them.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config\n",
    "    ):\n",
    "        \"\"\"\n",
    "            Automatically initialize the StagedTo3DConverter class by appying the configuration when an object of that class is created.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            self : need to explicitly state this parameter to pass any newly created object of class StagedTo3DConverter to the other paraneter (config)\n",
    "                this is a python syntax requirement in order for the object to persist of this class\n",
    "\n",
    "            config : dict or str\n",
    "                A dictionary of configuration settings or a path to a config\n",
    "                JSON file. (See help(pdgstaging.ConfigManager))\n",
    "\n",
    "            Notes\n",
    "            ----------\n",
    "            - this function does not do the staging or tiling steps\n",
    "        \"\"\"\n",
    "\n",
    "        self.config = pdgstaging.ConfigManager(config)\n",
    "        self.tiles = pdgstaging.TilePathManager(\n",
    "            **self.config.get_path_manager_config())\n",
    "\n",
    "    def all_staged_to_3dtiles(\n",
    "        self\n",
    "    ):\n",
    "        \"\"\"\n",
    "            Process all staged vector tiles into 3D tiles. This is simply a loop that iterates the function staged_to_rdtile() over all files in the staged directory.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the list of staged vector tiles\n",
    "        paths = self.tiles.get_filenames_from_dir('staged')\n",
    "        # Process each tile\n",
    "        for path in paths:\n",
    "            self.staged_to_3dtile(path)\n",
    "\n",
    "    def staged_to_3dtile(self, path):\n",
    "        \"\"\"\n",
    "            Convert a staged vector tile into a B3DM tile file and a matching\n",
    "            JSON tileset file.\n",
    "            - the B3DM tile is applied to the PDG portal for visualization purposes\n",
    "            - the JSON serves as the metadata for that tile\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            path : str\n",
    "                The path to the staged vector tile.\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            tile, tileset : Cesium3DTile, Tileset\n",
    "                The Cesium3DTiles and Cesium3DTileset objects\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            \n",
    "            # Get information about the tile from the path\n",
    "            tile = self.tiles.tile_from_path(path)\n",
    "            out_path = self.tiles.path_from_tile(tile, '3dtiles')\n",
    "\n",
    "            tile_bv = self.bounding_region_for_tile(tile) # bv = bounding volumne\n",
    "\n",
    "            # Get the filename of the tile WITHOUT the extension\n",
    "            tile_filename = os.path.splitext(os.path.basename(out_path))[0]\n",
    "            # Get the base of the path, without the filename\n",
    "            tile_dir = os.path.dirname(out_path) + os.path.sep\n",
    "\n",
    "            # Log the event\n",
    "            logger.info(\n",
    "                f'Creating 3dtile from {path} for tile {tile} to {out_path}.')\n",
    "\n",
    "            # Read in the staged vector tile\n",
    "            gdf = gpd.read_file(path)\n",
    "\n",
    "            # Summary of following steps:\n",
    "            # Now that we have the path to the staged vector tile esptablished and logged, \n",
    "            # the following checks are executed on each staged vector tile:\n",
    "            # 1. check if the tile has any data to start with\n",
    "            # 2. check if the centroid of the polygons within the tile are within the tile boundaries, remove if not\n",
    "            # 3. check if polygons within the tile overlap, deduplicate them if they do\n",
    "            # 4. check if the tile has any data left if deduplication was executed\n",
    "            # 5. if there were errors in the above steps, log that for debugging\n",
    "\n",
    "            \n",
    "            # Check if the gdf is empty\n",
    "            if len(gdf) == 0:\n",
    "                logger.warning(\n",
    "                    f'Vector tile {path} is empty. 3D tile will not be'\n",
    "                    ' created.')\n",
    "                return\n",
    "\n",
    "            # Remove polygons with centroids that are outside the tile boundary\n",
    "            prop_cent_in_tile = self.config.polygon_prop(\n",
    "                'centroid_within_tile')\n",
    "            gdf = gdf[gdf[prop_cent_in_tile]]\n",
    "\n",
    "            # Check if deduplication should be performed\n",
    "            dedup_here = self.config.deduplicate_at('3dtiles')\n",
    "            dedup_method = self.config.get_deduplication_method()\n",
    "\n",
    "            # Deduplicate if required\n",
    "            if dedup_here and (dedup_method is not None):\n",
    "                dedup_config = self.config.get_deduplication_config(gdf)\n",
    "                dedup = dedup_method(gdf, **dedup_config)\n",
    "                gdf = dedup['keep']\n",
    "\n",
    "                # The tile could theoretically be empty after deduplication\n",
    "                if len(gdf) == 0:\n",
    "                    logger.warning(\n",
    "                        f'Vector tile {path} is empty after deduplication.'\n",
    "                        ' 3D Tile will not be created.')\n",
    "                    return\n",
    "\n",
    "            # Create & save the b3dm file\n",
    "            ces_tile, ces_tileset = TreeGenerator.leaf_tile_from_gdf(\n",
    "                gdf,\n",
    "                dir=tile_dir,\n",
    "                filename=tile_filename,\n",
    "                z=self.config.get('z_coord'),\n",
    "                geometricError=self.config.get('geometricError'),\n",
    "                tilesetVersion=self.config.get('version'),\n",
    "                boundingVolume=tile_bv\n",
    "            )\n",
    "\n",
    "            return ces_tile, ces_tileset\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f'Error creating 3D Tile from {path}.')\n",
    "            logger.error(e)\n",
    "\n",
    "    def parent_3dtiles_from_children(self, tiles, bv_limit=None):\n",
    "        \"\"\"\n",
    "            Create parent Cesium 3D Tileset json files that point to\n",
    "            of child JSON files in the tile tree hierarchy.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            tiles : list of morecantile.Tile\n",
    "                The list of tiles to create parent tiles for.\n",
    "        \"\"\"\n",
    "\n",
    "        tile_manager = self.tiles\n",
    "        config_manager = self.config\n",
    "\n",
    "        tileset_objs = []\n",
    "\n",
    "        # Make the next level of parent tiles\n",
    "        for parent_tile in tiles:\n",
    "            # Get the path to the parent tile\n",
    "            parent_path = tile_manager.path_from_tile(parent_tile, '3dtiles')\n",
    "            # Get just the base dir without the filename\n",
    "            parent_dir = os.path.dirname(parent_path)\n",
    "            # Get the filename of the parent tile, without the extension\n",
    "            parent_filename = os.path.basename(parent_path)\n",
    "            parent_filename = os.path.splitext(parent_filename)[0]\n",
    "            # Get the children paths for this parent tile\n",
    "            child_paths = tile_manager.get_child_paths(parent_tile, '3dtiles')\n",
    "            # Remove paths that do not exist\n",
    "            child_paths = tile_manager.remove_nonexistent_paths(child_paths)\n",
    "            # Get the parent bounding volume\n",
    "            parent_bv = self.bounding_region_for_tile(\n",
    "                parent_tile, limit_to=bv_limit)\n",
    "            # If the bounding region is outside t\n",
    "            # Get the version\n",
    "            version = config_manager.get('version')\n",
    "            # Get the geometric error\n",
    "            geometric_error = config_manager.get('geometricError')\n",
    "            # Create the parent tile\n",
    "            tileset_obj = TreeGenerator.parent_tile_from_children_json(\n",
    "                child_paths,\n",
    "                dir=parent_dir,\n",
    "                filename=parent_filename,\n",
    "                geometricError=geometric_error,\n",
    "                tilesetVersion=version,\n",
    "                boundingVolume=parent_bv\n",
    "            )\n",
    "            tileset_objs.append(tileset_obj)\n",
    "\n",
    "        return tileset_objs\n",
    "\n",
    "    def bounding_region_for_tile(self, tile, limit_to=None):\n",
    "        \"\"\"\n",
    "        For a morecantile.Tile object, return a BoundingVolumeRegion object\n",
    "        that represents the bounding region of the tile.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        tile : morecantile.Tile\n",
    "            The tile object.\n",
    "        limit_to : list of float\n",
    "            Optional list of west, south, east, north coordinates to limit\n",
    "            the bounding region to.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bv : BoundingVolumeRegion\n",
    "            The bounding region object.\n",
    "        \"\"\"\n",
    "        tms = self.tiles.tms\n",
    "        bounds = tms.bounds(tile)\n",
    "        bounds = gpd.GeoSeries(\n",
    "            box(bounds.left, bounds.bottom, bounds.right, bounds.top),\n",
    "            crs=tms.crs)\n",
    "        if limit_to is not None:\n",
    "            bounds_limitor = gpd.GeoSeries(\n",
    "                box(limit_to[0], limit_to[1], limit_to[2], limit_to[3]),\n",
    "                crs=tms.crs)\n",
    "            bounds = bounds.intersection(bounds_limitor)\n",
    "        bounds = bounds.to_crs(BoundingVolumeRegion.CESIUM_EPSG)\n",
    "        bounds = bounds.total_bounds\n",
    "\n",
    "        region_bv = {\n",
    "            'west': bounds[0], 'south': bounds[1],\n",
    "            'east': bounds[2], 'north': bounds[3],\n",
    "        }\n",
    "        return region_bv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Configuring the stager, raster tiler, and 3D tiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# staging configuration\n",
    "stager = pdgstaging.TileStager(workflow_config)\n",
    "tile_manager = stager.tiles\n",
    "config_manager = stager.config\n",
    "\n",
    "# zoom levels configuration\n",
    "min_z = config_manager.get_min_z()\n",
    "max_z = config_manager.get_max_z()\n",
    "parent_zs = range(max_z - 1, min_z - 1, -1)\n",
    "\n",
    "# 3D tiler configuration\n",
    "tiles3dmaker = StagedTo3DConverter(workflow_config)\n",
    "\n",
    "# raster tilerconfiguration \n",
    "rasterizer = pdgraster.RasterTiler(workflow_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Stage the input files\n",
    "\n",
    "By staging the files using the stager we configured with the .json script, we created a `staged` directory of the tiles in a deliberate hierarchial structure. Each layer of the directory is as follows:\n",
    "- **staged**: base folder for all tiles (lakes)\n",
    "- **WorldCRS84Quad**: the tile matrix set grid, which is in geographic coordinates, allowing the tiles to appear square when represented on the 3D Globe on the PDG web portal (in Cesium format)\n",
    "- **11**: style (number of zoom levels, the \"z-range\")\n",
    "- **numbered subfolders, for example 406-481**: tile matrix (x)\n",
    "- **numbered tiles, for example 228.gpkg:** tile column (y)\n",
    "\n",
    "See [here](https://github.com/PermafrostDiscoveryGateway/viz-staging/blob/main/docs/tile_path_structure.md) for a schematic of this hierarchial directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in data_paths:\n",
    "    stager.stage(path)\n",
    "    # took 50 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Deduplicate and rasterize tiles in staged directory\n",
    "\n",
    "- This step rasterizes all staged tiles, and does not require an input path because the `staged` directory is defined in the .json configuration.\n",
    "- This is a more convenient function to call instead of rasterize() which would be used for batches in parallelization, because rasterize_all() does multiple steps in one! It dedeuplicates the tiles, rasterizes them, creates geotiffs for all z-levels, and web tiles for all z-levels (and `update_ranges = T`)\n",
    "- deduplicating the staged files needs to happen again when we create the 3d tiles, because those also pull from the staged directory, rather than the rasterized files that are deduplicated in the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rasterizer.rasterize_all()\n",
    "# with the sample data, this step took 1.5 hours because no parallelization has been implemented yet (parsl will be integrated in time!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Create parent geotiffs for all z-levels (except highest)\n",
    "\n",
    "- This step would only be necessary if we processed the staged vectors in parallel (batches) with rasterize(). But because this workflow used rasterier.rasterize_all(), we do not need to execute this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_composite_geotiffs(tiles, config, logging_dict=None):\n",
    "#     \"\"\"\n",
    "#     Make composite geotiffs (step 3)\n",
    "#     \"\"\"\n",
    "#     import pdgraster\n",
    "#     if logging_dict:\n",
    "#         import logging.config\n",
    "#         logging.config.dictConfig(logging_dict)\n",
    "#     rasterizer = pdgraster.RasterTiler(config) \n",
    "#     return rasterizer.parent_geotiffs_from_children(tiles, recursive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for z in parent_zs: \n",
    "\n",
    "#     # Determine which tiles we need to make for the next z-level based on the\n",
    "#     # path names of the files just created\n",
    "#     child_paths = tile_manager.get_filenames_from_dir('geotiff', z=z + 1)\n",
    "#     parent_tiles = set()\n",
    "#     for child_path in child_paths:\n",
    "#         parent_tile = tile_manager.get_parent_tile(child_path)\n",
    "#         parent_tiles.add(parent_tile)\n",
    "#     parent_tiles = list(parent_tiles)\n",
    "\n",
    "#     # composite_geotiffs = []\n",
    "#     # for parent_tile in parent_tiles:\n",
    "#     #     composite_geotiff = create_composite_geotiffs(\n",
    "#     #         parent_tiles, workflow_config, logging_dict)\n",
    "#     #     composite_geotiffs.append(composite_geotiff)\n",
    "\n",
    "#     # [a.result() for a in composite_geotiffs]\n",
    "\n",
    "#     create_composite_geotiffs(tiles = parent_tiles, config = workflow_config, logging_dict = logging_dict)   \n",
    "            \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Create web tiles from geotiffs\n",
    "\n",
    "- This step would only be necessary if we processed the staged vectors in parallel (batches) with rasterize(). But because this workflow used rasterier.rasterize_all(), we do not need to execute this step. It was already done under the hood by rasterize_all() calling webtiles_from_all_geotiffs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rasterizer.update_ranges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geotiff_paths = tile_manager.get_filenames_from_dir('geotiff')\n",
    "# len(geotiff_paths)\n",
    "# geotiff_paths[7767]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_web_tiles(geotiff_paths, config, logging_dict=None):\n",
    "#     \"\"\"\n",
    "#     Create a batch of webtiles from geotiffs (step 4)\n",
    "#     \"\"\"\n",
    "#     import pdgraster\n",
    "#     if logging_dict:\n",
    "#         import logging.config\n",
    "#         logging.config.dictConfig(logging_dict)\n",
    "#     rasterizer = pdgraster.RasterTiler(config)\n",
    "#     return rasterizer.webtiles_from_geotiffs(\n",
    "#         geotiff_paths, update_ranges=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_web_tiles(geotiff_paths, workflow_config, logging_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process web tiles NOT in batches\n",
    "#rasterizer.webtiles_from_geotiffs(geotiff_paths, update_ranges=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Deduplicate and make leaf 3D tiles (only highest z-level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staged_paths = stager.tiles.get_filenames_from_dir('staged')\n",
    "\n",
    "# define function to create leaf 3d tiles\n",
    "def create_leaf_3dtiles(staged_paths, config, logging_dict=None):\n",
    "    \"\"\"\n",
    "    Create a leaf 3d tiles from staged vector tiles (this is not done in batches in this script, but is in batches in the parsl workflow)\n",
    "    \"\"\"\n",
    "    \n",
    "    if logging_dict:\n",
    "        import logging.config\n",
    "        logging.config.dictConfig(logging_dict)\n",
    "    converter3d = StagedTo3DConverter(config)\n",
    "    tilesets = []\n",
    "    for path in staged_paths:\n",
    "        try:\n",
    "            ces_tile, ces_tileset = converter3d.staged_to_3dtile(path)\n",
    "            tilesets.append(ces_tileset)\n",
    "        except Exception as e:\n",
    "            logging.error(f'Error creating 3d tile from {path}')\n",
    "            logging.error(e)\n",
    "    return tilesets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(items, batch_size):\n",
    "    \"\"\"\n",
    "    Create batches of a given size from a list of items.\n",
    "    \"\"\"\n",
    "    return [items[i:i + batch_size] for i in range(0, len(items), batch_size)]\n",
    "\n",
    "batch_size_3dtiles = 20\n",
    "batch_size_parent_3dtiles = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converter3d = StagedTo3DConverter(workflow_config)\n",
    "#converter3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tiles3dmaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that staged_paths is a list of the staged vectors\n",
    "staged_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staged_batches = make_batch(staged_paths, batch_size_3dtiles)\n",
    "\n",
    "for batch in staged_batches:\n",
    "    create_leaf_3dtiles(staged_paths = staged_paths, config = workflow_config, logging_dict = logging_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete this chunk if above code runs correctly \n",
    "# alternative to running the function defined above, create_leaf_3dtiles():\n",
    "#tiles3dmaker.all_staged_to_3dtiles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Create parent Cesium 3D tilesets for all z-levels (except highest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_z_tiles = [tile_manager.tile_from_path(path) for path in staged_paths]\n",
    "# get the total bounds for all the tiles\n",
    "max_z_bounds = [tile_manager.get_bounding_box(tile) for tile in max_z_tiles]\n",
    "# get the total bounds for all the tiles\n",
    "polygons = [box(bounds['left'],\n",
    "                bounds['bottom'],\n",
    "                bounds['right'],\n",
    "                bounds['top']) for bounds in max_z_bounds]\n",
    "max_z_bounds = gpd.GeoSeries(polygons, crs=tile_manager.tms.crs)\n",
    "\n",
    "bound_volume_limit = max_z_bounds.total_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for z in parent_zs:\n",
    "\n",
    "    # Determine which tiles we need to make for the next z-level based on the\n",
    "    # path names of the files just created\n",
    "    all_child_paths = tiles3dmaker.tiles.get_filenames_from_dir('3dtiles', z=z + 1)\n",
    "\n",
    "    parent_tiles = set()\n",
    "    for child_path in all_child_paths:\n",
    "        parent_tile = tile_manager.get_parent_tile(child_path)\n",
    "        parent_tiles.add(parent_tile)\n",
    "    parent_tiles = list(parent_tiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_child_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parent_3dtiles(tiles, config, limit_bv_to=None, logging_dict=None):\n",
    "    \"\"\"\n",
    "    Create a batch of cesium 3d tileset parent files that point to child\n",
    "    tilesets\n",
    "    \"\"\"\n",
    "    #from pdg_workflow import StagedTo3DConverter\n",
    "    if logging_dict:\n",
    "        import logging.config\n",
    "        logging.config.dictConfig(logging_dict)\n",
    "    converter3d = StagedTo3DConverter(config)\n",
    "    return converter3d.parent_3dtiles_from_children(tiles, limit_bv_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_parent_3dtiles(parent_tiles, workflow_config, bound_volume_limit, logging_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('pdgviz': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b54da58c51b30b82a7fc0059a0ef455812aa5d7a05176ea08a9c6d431b69c979"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
